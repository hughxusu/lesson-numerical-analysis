# 幂法与反幂法

在实际问题中，矩阵的模最大特征值往往起主要作用，例如矩阵的谱半径就是矩阵的模最大特征值，它决定了迭代矩阵是否收敛。因此矩阵的模最大特征值比其他特征值的地位更加重要。

幂法就是计算矩阵的模最大特征值及其特征向量的数值方法。

反幂法就是计算矩阵的模最小特征值及其特征向量的数值方法。

计算矩阵特征值和特征向量的一般步骤：

1. 计算特征多项式，即$\lambda E-A$的行列值。
1. 计算特征多项式的根，$\det(\lambda E-A)$的$n$个跟。
1. 将所求的根逐个代入方程组中，所有解的全体组成$A$。

## 幂法

设$n$阶方阵$A$，任取初始向量$X^{(0)}$，进行迭代计算
$$
X^{(k+1)}=AX^{k} \tag{1}
$$
得到迭代序列$\left \{ X^{(k)} \right \}$，$k=1,2,\cdots,3$，分析$X^{(k+1)}$与$X^{k}$的关系，就可以得到$A$的模最大特征值。

设矩阵$A$有特征值$\lambda_i$，$i=1,2,\cdots,3$，其中$\left| \lambda_1 \right| \ge \left| \lambda_2 \right|\ge \cdots \ge \left| \lambda_n \right|$，并且有$n$个线性无关的特征向量$v_i$。即：
$$
Av_i=\lambda_iv_i \quad i=1,2,\cdots, n
$$
任取初始向量$X^{(0)}$，$X^{(0)}$可以表示成$A$的$n$个线性无关的特征向量的线性组合，即：
$$
X^{(0)}=a_1v_1+a_2v_2+\cdots+a_nv_n
$$
根据公式 $(1)$ 有
$$
\begin{aligned} 
X^{1}=AX^{0} 
&= a_1Av_1+a_2Av_2+\cdots+a_nAv_n \\
&= a_1\lambda_1v_1+a_2\lambda_2v_2+\cdots+a_n\lambda_nv_n
\end{aligned}
$$
从而
$$
\begin{aligned} 
X^{(k)}&=AX^{(k-1)}=A^kX^{(0)} \\
&=a_1\lambda_1^kv_1+a_2\lambda_2^kv_2+\cdots+a_n\lambda_n^kv_n \\
&=\lambda_1^k\left(a_1v_1+a_2\frac{\lambda_2^k}{\lambda_1^k}v_2+\cdots+a_n\frac{\lambda_n^k}{\lambda_1^k}v_n\right) \\
&=\lambda_1^k\left(a_1v_1+\sum_{i=2}^na_iv_i\left( \frac{\lambda_i}{\lambda_1}\right)^k\right) 
\end{aligned}
$$
由于
$$
\because\left|\frac{\lambda_i}{\lambda_1}\right|<1 \qquad
\therefore \lim_{k\to\infty}\left|\frac{\lambda_i}{\lambda_1}\right|^k=0
$$
则k充分大时
$$
X^{(k)}\approx a_1\lambda_1^kv_1
$$
可作为与$\lambda_1$相对应的特征向量的近似向量，原因是
$$
\begin{aligned} 
AX^{(k)} &\approx A\left(a_1\lambda_1^kv_1\right) = a_1\lambda_1^{k+1}v_1= \lambda_1\left(a_1\lambda_1^{k}v_1\right) \\
&\approx\lambda_1X^{(k)}
\end{aligned}
$$
即$X^{(k)}$是特征向量，$\lambda_1$为特征值。
