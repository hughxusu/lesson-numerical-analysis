# 幂法与反幂法

在实际问题中，矩阵的模最大特征值往往起主要作用，例如矩阵的谱半径就是矩阵的模最大特征值，它决定了迭代矩阵是否收敛。因此矩阵的模最大特征值比其他特征值的地位更加重要。

* 幂法就是计算矩阵的模最大特征值及其特征向量的数值方法。
* 反幂法就是计算矩阵的模最小特征值及其特征向量的数值方法。

计算矩阵特征值和特征向量的一般步骤：

1. 计算特征多项式，即$\lambda E-A$的行列值。
1. 计算特征多项式的根，$\det(\lambda E-A)$的$n$个根。
1. 将所求的根逐个代入方程组中，所有解的全体组成$A$。

## 幂法

设$n$阶方阵$A$，任取初始向量$X^{(0)}$，进行迭代计算
$$
X^{(k+1)}=AX^{(k)} \tag{1}
$$
得到迭代序列$\left \{ X^{(k)} \right \}$，$k=1,2,\cdots,3$，分析$X^{(k+1)}$与$X^{k}$的关系，就可以得到$A$的模最大特征值。

设矩阵$A$有特征值$\lambda_i$，$i=1,2,\cdots,3$，其中$\left| \lambda_1 \right| \ge \left| \lambda_2 \right|\ge \cdots \ge \left| \lambda_n \right|$，并且有$n$个线性无关的特征向量$v_i$。即：
$$
Av_i=\lambda_iv_i \quad i=1,2,\cdots, n
$$
任取初始向量$X^{(0)}$，$X^{(0)}$可以表示成$A$的$n$个线性无关的特征向量的线性组合，即：
$$
X^{(0)}=a_1v_1+a_2v_2+\cdots+a_nv_n
$$
根据公式 $(1)$ 有
$$
\begin{aligned} 
X^{(1)}=AX^{(0)} 
&= a_1Av_1+a_2Av_2+\cdots+a_nAv_n \\
&= a_1\lambda_1v_1+a_2\lambda_2v_2+\cdots+a_n\lambda_nv_n
\end{aligned}
$$
从而
$$
\begin{aligned} 
X^{(k)}&=AX^{(k-1)}=A^kX^{(0)} \\
&=a_1\lambda_1^kv_1+a_2\lambda_2^kv_2+\cdots+a_n\lambda_n^kv_n \\
&=\lambda_1^k\left(a_1v_1+a_2\frac{\lambda_2^k}{\lambda_1^k}v_2+\cdots+a_n\frac{\lambda_n^k}{\lambda_1^k}v_n\right) \\
&=\lambda_1^k\left(a_1v_1+\sum_{i=2}^na_iv_i\left( \frac{\lambda_i}{\lambda_1}\right)^k\right) 
\end{aligned}
$$
由于
$$
\because\left|\frac{\lambda_i}{\lambda_1}\right|<1 \qquad
\therefore \lim_{k\to\infty}\left|\frac{\lambda_i}{\lambda_1}\right|^k=0
$$
则k充分大时
$$
X^{(k)}\approx a_1\lambda_1^kv_1
$$
可作为与$\lambda_1$相对应的特征向量的近似向量，原因是
$$
\begin{aligned} 
AX^{(k)} &\approx A\left(a_1\lambda_1^kv_1\right) = a_1\lambda_1^{k+1}v_1= \lambda_1\left(a_1\lambda_1^{k}v_1\right) \\
&\approx\lambda_1X^{(k)}
\end{aligned}
$$
即$X^{(k)}$是特征向量，$\lambda_1$为特征值。

> [!warning]
>
> $AX^{(k)}=\lambda_1X^{(k)}$要求每个分量都相等。

设$v_1\ne0$为求$\lambda_1$，观查
$$
\frac{\left(X^{(k)}\right)_j}{\left(X^{(k-1)}\right)_j}=
\lambda_1\frac{\left(a_1\left(v_1\right)_j+\sum_{i=2}^na_i\left(v_i\right)_j\left(\frac{\lambda_i}{\lambda_1}\right)^k\right)}
{\left(a_1\left(v_1\right)_j+\sum_{i=2}^na_i\left(v_i\right)_j\left(\frac{\lambda_i}{\lambda_1}\right)^{k-1}\right)} 
$$
所以
$$
\left|\frac{\lambda_i}{\lambda_1}\right|<1\Rightarrow 
\lim_{k\to\infty}\frac{\left(X^{(k)}\right)_j}{\left (X^{(k-1)}\right)_j}=\lambda_1
$$
所以$k$充分大时
$$
\frac{\left(X^{(k)}\right)_j}{\left (X^{(k-1)}\right)_j}\approx\lambda_1
$$
即
$$
\lambda_1\approx\frac{X_i^{(k)}}{X^{(k-1)}_i} \qquad i=1,2,\cdots,n
$$
解题步骤：

1. 任给$n$维初始向量$X^{(0)}\ne0$；
2. 按$X^{(k)}=AX^{(k-1)}=\cdots=A^kX^{(0)}$计算$X^{(k)}$
3. 如果$k$从某个数后分量比为常数

$$
\frac{\left(X^{(k)}\right)_j}{\left (X^{(k-1)}\right)_j}\approx c
$$

则取$\lambda_1\approx c$，而$X^{(k)}$就是与$\lambda_1$对应的一个近似特征向量。

### 初始向量旋转

如果$x^{(0)}$的选取使得$a_i=0$，幂法计算仍能进行。因为计算过程中舍入误差的影响，迭代若干次后，必然会产生一个向量$x^{(k)}$，它在$v_i$方向上的分量不为零，这样，以后的计算就满足所设条件。

> [!warning]
>
> 由初始向量的任意性，选取其它不为零的初始向量。

### 幂法的规范化

为避免$\left\|x^{(k)}\right\|$ 过大或过小，幂法可按下面的规范运算进行
$$
\left\{\begin{matrix}
X^{(k+1)}=AY^{(k)} \\
Y^{(k+1)}=\frac{X^{(k+1)}}{\left\|X^{(k+1)}\right\|_\infty} 
\end{matrix}\right.
\qquad k=0,1,2\cdots
$$
其中
$$
Y^{(0)}=\frac{X^{(0)}}{\left\|X^{(0)}\right\|_\infty}
$$

### 多重根

**$\lambda_1$为重根情况**：前面假定$\left| \lambda_1 \right| \ge \left| \lambda_2 \right|$，如果按照模最大的特征值有多个，即
$$
\left| \lambda_1 \right| = \left| \lambda_1 \right|=\cdots=\left| \lambda_1 \right| \ge \left| \lambda_2 \right|\ge \cdots \ge \left| \lambda_n \right|
$$
$\lambda_1$是$m$重根，即$\lambda_1=\lambda_2=\cdots=\lambda_m$，矩阵$A$仍有$n$个线性无关的特征向量。此时有
$$
x^{(k+1)}=\lambda_1^{k+1}\left[
a_1v_1+\cdots+a_mv_m+\left(\frac{\lambda_{m+1}}{\lambda_1}\right)^{k+1}a_{m+1}v_{m+1}
+\cdots+\left(\frac{\lambda_{n}}{\lambda_1}\right)^{k+1}a_{n}v_{n}
\right]
$$
显然，只要$a_1,\cdots,a_m$不全为零，当$k$充分大时，就有
$$
x^{(k+1)}\approx\lambda_1^{k+1}\left(a_1v_1+\cdots+a_mv_m\right)
$$
因$a_1v_1+\cdots+a_mv_m$也是矩阵$A$相应于$\lambda_1$的特征向量，故有
$$
\lambda_1=\lambda_2=\cdots=\lambda_m\approx\frac{x^{(k+1)}_i}{x^{(k)}_i}
$$
$x^{(k+1)}$为相应的特征向量，即对这种情况幂法仍然有效。

> [!note]
>
> 用幂法求矩阵$A$按模最大特征值$\lambda_1$和对应的特征向量$x_1$。

$$
A=\begin{pmatrix}
1 & 1 & 0.5 \\
1 & 1  & 0.25 \\
0.5 & 0.25 & 2
\end{pmatrix}
$$

取初始向量$v_0=\left(1,1,1\right)^T$

1. 第一次迭代

$$
\begin{align*}
v^{(1)} &= Au^{(0)} = 
\begin{pmatrix}
1 \cdot 1 + 1 \cdot 1 + 0.5 \cdot 1 \\
1 \cdot 1 + 1 \cdot 1 + 0.25 \cdot 1 \\
0.5 \cdot 1 + 0.25 \cdot 1 + 2 \cdot 1
\end{pmatrix} = 
\begin{pmatrix} 2.5 \\ 2.25 \\ 2.75 \end{pmatrix} 
\\ \\
\lambda^{(1)} &= \|v^{(1)}\|_\infty = 2.75 
\\ \\
u^{(1)} &= \frac{v^{(1)}}{2.75} \approx 
\begin{pmatrix} 0.9091 \\ 0.8182 \\ 1 \end{pmatrix}
\end{align*}
$$

2. 第二次迭代

$$
\begin{align*}
v^{(2)} &= Au^{(1)} \approx 
\begin{pmatrix}
1 \cdot 0.9091 + 1 \cdot 0.8182 + 0.5 \cdot 1 \\
1 \cdot 0.9091 + 1 \cdot 0.8182 + 0.25 \cdot 1 \\
0.5 \cdot 0.9091 + 0.25 \cdot 0.8182 + 2 \cdot 1
\end{pmatrix} \approx 
\begin{pmatrix} 2.2273 \\ 1.9773 \\ 2.659 \end{pmatrix} 
\\ \\
\lambda^{(2)} &= \|v^{(2)}\|_\infty = 2.659 
\\ \\
u^{(2)} &\approx \frac{v^{(2)}}{2.659} \approx 
\begin{pmatrix} 0.8376 \\ 0.7435 \\ 1 \end{pmatrix}
\end{align*}
$$

3. 第三次迭代

$$
\begin{align*}
v^{(3)} &= Au^{(2)} \approx 
\begin{pmatrix}
1 \cdot 0.8376 + 1 \cdot 0.7435 + 0.5 \cdot 1 \\
1 \cdot 0.8376 + 1 \cdot 0.7435 + 0.25 \cdot 1 \\
0.5 \cdot 0.8376 + 0.25 \cdot 0.7435 + 2 \cdot 1
\end{pmatrix} \approx 
\begin{pmatrix} 2.0811 \\ 1.8311 \\ 2.6047 \end{pmatrix} 
\\ \\
\lambda^{(3)} &= \|v^{(3)}\|_\infty = 2.6047 
\\ \\
u^{(3)} &\approx \frac{v^{(3)}}{2.6047} \approx 
\begin{pmatrix} 0.7992 \\ 0.7031 \\ 1 \end{pmatrix}
\end{align*}
$$

经过31次迭代$\lambda\approx2.5365$，$u≈\left(0.5315, 0.4615, 0.7103\right)^T$

幂法的Python计算程序

```python
import numpy as np

def power_method(A, x0, tol=1e-8, max_iter=1000):
    if A.shape[0] != A.shape[1]:
        raise ValueError("矩阵A必须是方阵")
    if A.shape[0] != x0.shape[0]:
        raise ValueError("初始向量x0的维度与矩阵A不一致")

    x = x0.astype(float)
    x /= np.linalg.norm(x)
    
    for k in range(max_iter):
        x_new = A @ x
        norm = np.linalg.norm(x_new)
        if norm == 0:
            raise ValueError("幂法失败：遇到零向量")
        x_new /= norm
        
        if np.linalg.norm(x_new - x) < tol:
            lambda_max = x_new.T @ A @ x_new
            return lambda_max, x_new, k + 1
        x = x_new

    raise ValueError("幂法未收敛，请增加最大迭代次数或调整初始向量")

A = np.array([
    [1, 1, 0.5],
    [1, 1, 0.25],
    [0.5, 0.25, 2]
])
x0 = np.array([1, 1, 1])
eigenvalue, eigenvector, iterations = power_method(A, x0)
print(f"最大特征值: {eigenvalue}")
print(f"对应特征向量: {eigenvector}")
print(f"迭代次数: {iterations}")
```

> [!warning]
>
> 幂法适用于求大型矩阵的模最大特征值$\lambda_1$和对应的特征向量$x_1$。

### 收敛速度

幂法的收敛速度依赖于比值
$$
\left|\frac{\lambda_2}{\lambda_1}\right|
$$
比值越小收敛越快，由此便提出了幂法的加速收敛方法：

* 点移位法。
* 瑞雷商加速法速法等。

## 反幂法

原理：设$A$为非奇异矩阵，那么$\lambda \ne 0$且$\frac{1}{\lambda}$为$A^{-1}$特征值，即
$$
A^{-1}x=\frac{1}{\lambda}x
$$
对应的特征向量仍是$x$。

> [!warning]
>
> $A$的模最小特征根的倒数，正式$A^{-1}$的模最大特征值。

反幂法：可求非奇异实矩阵$A$的按模最小特征值（即求$A$的逆最大特征根）及特征向量的数值方法。

因为$A^{-1}$的计算比较麻烦，且往往不能保证矩阵$A$有一些好的性质，如：稀疏性，因此反幂法在实际计算时，以求解方程组：
$$
Ax^{(k+1)}=x^{(k)}
$$
代替幂法迭代
$$
x^{(k+1)}=A^{-1}x^{(k)}
$$
**定理**

设$A\in R^{n\times n}$为对称矩阵，其特征值满足$\left| \lambda_1 \right| \ge \left| \lambda_2 \right|\ge \cdots \ge \left| \lambda_n \right|$，应用幂法或反幂法$u_k$的瑞雷商给出$\lambda_1$的较好近似
$$
\lambda(u_k)=\frac{u^TAu}{u^Tu}=\lambda_1+O\left(\left(\frac{\lambda_2}{\lambda_1}\right)^{2k}\right)
$$
反幂法求解的一般的过程，迭代形式
$$
\begin{align*}
Ay^{(k)} & =x^{(k)} 
\\ \\
x^{(k+1)} &= \frac{y^{(k)}}{\|y^{(k)}\|} 
\end{align*}
$$
计算出特征向量后使用瑞雷商可以计算出特征值。

> [!note]
>
> 使用反幂法来求解矩阵$A$的模最小特征值及其对应的特征向量。

$$
A = \begin{pmatrix}
2 & 1 & 0 \\
1 & 3 & 1 \\
0 & 1 & 4
\end{pmatrix}
$$

取初始向量$x_0=\left(1,1,1\right)^T$

1. 第一次迭代，解线性方程组

$$
\begin{pmatrix}
2 & 1 & 0 \\
1 & 3 & 1 \\
0 & 1 & 4
\end{pmatrix}
\begin{pmatrix}
y_1 \\ y_2 \\ y_3
\end{pmatrix}
=
\begin{pmatrix}
1 \\ 1 \\ 1
\end{pmatrix}
$$

解得
$$
\begin{align*} 
y^{(0)} &\approx \begin{pmatrix} -0.0714 \\ 1.1429 \\ -0.0357 \end{pmatrix}
\\ \\
{x}^{(1)} &= \frac{{y}^{(0)}}{\|{y}^{(0)}\|} \approx \begin{pmatrix} -0.0619 \\ 0.9905 \\ -0.0310 \end{pmatrix}
\end{align*}
$$

2. 第二次迭代

$$
\begin{align*} 
A{y}^{(1)} &= {x}^{(1)} \approx \begin{pmatrix} -0.0619 \\ 0.9905 \\ -0.0310 \end{pmatrix}
\\ \\
{y}^{(1)} &\approx \begin{pmatrix} 0.2074 \\ 0.4129 \\ -0.1103 \end{pmatrix}
\\ \\
{x}^{(2)} &= \frac{{y}^{(1)}}{\|{y}^{(1)}\|} \approx \begin{pmatrix} 0.435 \\ 0.865 \\ -0.231 \end{pmatrix}
\end{align*}
$$

使用瑞雷商估计特征值
$$
\lambda_{\min} \approx \frac{{x}^{(2)T} A {x}^{(2)}}{{x}^{(2)T} {x}^{(2)}} \approx 1.170
$$

## 作业

> [!tip]
>
> 教材第276页：第3题，第1问。

